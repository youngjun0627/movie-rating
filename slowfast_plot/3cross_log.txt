1.8.0
Loading dataset
train_dataset : batch_size -> 4, step_size -> 730, frames -> 640
validation_dataset : batch_size -> 4, step_size -> 82, frames -> 640
=========================================================================================================
Load model : mode -> multi, label_size -> 7, sub_class_num -> 3
train gogosing
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [100/730]
data_time: 0.043, batch_time: 0.784
Loss : 0.91074
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [200/730]
data_time: 12.769, batch_time: 13.533
Loss : 0.88798
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [300/730]
data_time: 0.046, batch_time: 0.782
Loss : 0.86496
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [400/730]
data_time: 0.041, batch_time: 0.781
Loss : 0.87106
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [500/730]
data_time: 0.035, batch_time: 0.761
Loss : 0.85068
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [600/730]
data_time: 0.048, batch_time: 0.777
Loss : 0.84909
----------------------------------------------
lr : 0.00025
Epoch:[1] step : [700/730]
data_time: 2.347, batch_time: 3.122
Loss : 0.84968
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [100/730]
data_time: 8.149, batch_time: 8.924
Loss : 0.85334
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [200/730]
data_time: 0.045, batch_time: 0.792
Loss : 0.84548
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [300/730]
data_time: 3.673, batch_time: 4.445
Loss : 0.84519
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [400/730]
data_time: 0.052, batch_time: 0.783
Loss : 0.83963
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [500/730]
data_time: 14.968, batch_time: 15.730
Loss : 0.86196
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [600/730]
data_time: 0.040, batch_time: 0.775
Loss : 0.84752
----------------------------------------------
lr : 0.0002270820393249937
Epoch:[2] step : [700/730]
data_time: 0.032, batch_time: 0.770
Loss : 0.84166
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [100/730]
data_time: 0.032, batch_time: 0.767
Loss : 0.84143
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [200/730]
data_time: 0.047, batch_time: 0.777
Loss : 0.82313
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [300/730]
data_time: 7.034, batch_time: 7.812
Loss : 0.82845
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [400/730]
data_time: 0.036, batch_time: 0.773
Loss : 0.85316
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [500/730]
data_time: 5.304, batch_time: 6.081
Loss : 0.84634
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [600/730]
data_time: 7.467, batch_time: 8.230
Loss : 0.84178
----------------------------------------------
lr : 0.00016708203932499368
Epoch:[3] step : [700/730]
data_time: 0.034, batch_time: 0.768
Loss : 0.83144
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [100/730]
data_time: 0.032, batch_time: 0.764
Loss : 0.84528
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [200/730]
data_time: 12.789, batch_time: 13.559
Loss : 0.83293
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [300/730]
data_time: 15.192, batch_time: 15.944
Loss : 0.82435
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [400/730]
data_time: 20.110, batch_time: 20.913
Loss : 0.83047
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [500/730]
data_time: 0.038, batch_time: 0.766
Loss : 0.82801
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [600/730]
data_time: 0.034, batch_time: 0.774
Loss : 0.83135
----------------------------------------------
lr : 9.29179606750063e-05
Epoch:[4] step : [700/730]
data_time: 0.058, batch_time: 0.799
Loss : 0.82862
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [100/730]
data_time: 0.040, batch_time: 0.776
Loss : 0.81478
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [200/730]
data_time: 19.908, batch_time: 20.696
Loss : 0.81985
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [300/730]
data_time: 0.052, batch_time: 0.796
Loss : 0.81853
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [400/730]
data_time: 11.453, batch_time: 12.218
Loss : 0.83000
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [500/730]
data_time: 0.036, batch_time: 0.768
Loss : 0.82242
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [600/730]
data_time: 5.288, batch_time: 6.048
Loss : 0.81117
----------------------------------------------
lr : 3.291796067500632e-05
Epoch:[5] step : [700/730]
data_time: 14.571, batch_time: 15.338
Loss : 0.82653
======================================================
validation gogosing
validation loss -> 0.81390, 	 f1_score -> 0.55115
======================================================
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [100/730]
data_time: 0.043, batch_time: 0.749
Loss : 0.80966
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [200/730]
data_time: 1.535, batch_time: 2.235
Loss : 0.80086
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [300/730]
data_time: 0.036, batch_time: 0.747
Loss : 0.80768
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [400/730]
data_time: 0.052, batch_time: 0.761
Loss : 0.80812
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [500/730]
data_time: 0.033, batch_time: 0.742
Loss : 0.80190
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [600/730]
data_time: 3.600, batch_time: 4.332
Loss : 0.81598
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [700/730]
data_time: 0.033, batch_time: 0.733
Loss : 0.80857
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [100/730]
data_time: 0.037, batch_time: 0.742
Loss : 0.81364
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [200/730]
data_time: 10.607, batch_time: 11.325
Loss : 0.80792
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [300/730]
data_time: 11.894, batch_time: 12.635
Loss : 0.81505
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [400/730]
data_time: 19.079, batch_time: 19.817
Loss : 0.81579
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [500/730]
data_time: 7.944, batch_time: 8.662
Loss : 0.80311
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [600/730]
data_time: 0.046, batch_time: 0.758
Loss : 0.80038
----------------------------------------------
lr : 3.2917960675006305e-05
Epoch:[7] step : [700/730]
data_time: 0.035, batch_time: 0.733
Loss : 0.82573
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [100/730]
data_time: 0.034, batch_time: 0.733
Loss : 0.82552
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [200/730]
data_time: 0.042, batch_time: 0.744
Loss : 0.82822
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [300/730]
data_time: 1.866, batch_time: 2.597
Loss : 0.83046
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [400/730]
data_time: 0.037, batch_time: 0.734
Loss : 0.81524
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [500/730]
data_time: 9.776, batch_time: 10.504
Loss : 0.81139
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [600/730]
data_time: 11.442, batch_time: 12.177
Loss : 0.80421
----------------------------------------------
lr : 9.291796067500633e-05
Epoch:[8] step : [700/730]
data_time: 0.034, batch_time: 0.735
Loss : 0.81451
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [100/730]
data_time: 10.673, batch_time: 11.388
Loss : 0.82624
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [200/730]
data_time: 5.286, batch_time: 6.029
Loss : 0.83355
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [300/730]
data_time: 10.611, batch_time: 11.365
Loss : 0.82267
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [400/730]
data_time: 0.047, batch_time: 0.748
Loss : 0.83041
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [500/730]
data_time: 0.059, batch_time: 0.768
Loss : 0.83758
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [600/730]
data_time: 0.033, batch_time: 0.735
Loss : 0.82944
----------------------------------------------
lr : 0.00016708203932499374
Epoch:[9] step : [700/730]
data_time: 0.031, batch_time: 0.728
Loss : 0.81502
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [100/730]
data_time: 0.043, batch_time: 0.752
Loss : 0.83067
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [200/730]
data_time: 0.032, batch_time: 0.740
Loss : 0.82129
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [300/730]
data_time: 0.033, batch_time: 0.741
Loss : 0.82312
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [400/730]
data_time: 2.595, batch_time: 3.310
Loss : 0.84143
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [500/730]
data_time: 24.394, batch_time: 25.145
Loss : 0.83217
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [600/730]
data_time: 14.796, batch_time: 15.537
Loss : 0.82505
----------------------------------------------
lr : 0.0002270820393249938
Epoch:[10] step : [700/730]
data_time: 0.048, batch_time: 0.753
Loss : 0.83316
======================================================
validation gogosing
validation loss -> 0.82602, 	 f1_score -> 0.58204
======================================================
----------------------------------------------
lr : 0.0002500000000000001
Epoch:[11] step : [100/730]
data_time: 0.050, batch_time: 0.751
Loss : 0.82263
----------------------------------------------
lr : 0.0002500000000000001
Epoch:[11] step : [200/730]
data_time: 0.036, batch_time: 0.742
Loss : 0.82642
----------------------------------------------
lr : 0.0002500000000000001
Epoch:[11] step : [300/730]
data_time: 0.035, batch_time: 0.732
Loss : 0.83034
----------------------------------------------
lr : 0.0002500000000000001
Epoch:[11] step : [400/730]
data_time: 0.034, batch_time: 0.733
Loss : 0.82912
