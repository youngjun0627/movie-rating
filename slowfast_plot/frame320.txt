1.8.0
Loading dataset
train_dataset : batch_size -> 8, step_size -> 429, frames -> 320
validation_dataset : batch_size -> 8, step_size -> 48, frames -> 320
=========================================================================================================
Load model : mode -> multi, label_size -> 7, sub_class_num -> 3
train gogosing
----------------------------------------------
lr : 0.001
Epoch:[1] step : [100/429]
data_time: 7.035, batch_time: 7.748
Loss : 1.02473
----------------------------------------------
lr : 0.001
Epoch:[1] step : [200/429]
data_time: 0.062, batch_time: 0.744
Loss : 0.98357
----------------------------------------------
lr : 0.001
Epoch:[1] step : [300/429]
data_time: 0.041, batch_time: 0.722
Loss : 0.97722
----------------------------------------------
lr : 0.001
Epoch:[1] step : [400/429]
data_time: 0.035, batch_time: 0.720
Loss : 0.97770
----------------------------------------------
lr : 0.000905463412215599
Epoch:[2] step : [100/429]
data_time: 0.045, batch_time: 0.730
Loss : 0.97028
----------------------------------------------
lr : 0.000905463412215599
Epoch:[2] step : [200/429]
data_time: 0.046, batch_time: 0.734
Loss : 0.96990
----------------------------------------------
lr : 0.000905463412215599
Epoch:[2] step : [300/429]
data_time: 0.035, batch_time: 0.713
Loss : 0.96806
----------------------------------------------
lr : 0.000905463412215599
Epoch:[2] step : [400/429]
data_time: 0.035, batch_time: 0.717
Loss : 0.97446
----------------------------------------------
lr : 0.000657963412215599
Epoch:[3] step : [100/429]
data_time: 2.292, batch_time: 3.004
Loss : 0.96299
----------------------------------------------
lr : 0.000657963412215599
Epoch:[3] step : [200/429]
data_time: 0.056, batch_time: 0.743
Loss : 0.96502
----------------------------------------------
lr : 0.000657963412215599
Epoch:[3] step : [300/429]
data_time: 0.042, batch_time: 0.723
Loss : 0.96767
----------------------------------------------
lr : 0.000657963412215599
Epoch:[3] step : [400/429]
data_time: 0.050, batch_time: 0.733
Loss : 0.96332
----------------------------------------------
lr : 0.0003520365877844011
Epoch:[4] step : [100/429]
data_time: 0.039, batch_time: 0.726
Loss : 0.95814
----------------------------------------------
lr : 0.0003520365877844011
Epoch:[4] step : [200/429]
data_time: 0.046, batch_time: 0.735
Loss : 0.95895
----------------------------------------------
lr : 0.0003520365877844011
Epoch:[4] step : [300/429]
data_time: 0.038, batch_time: 0.725
Loss : 0.95748
----------------------------------------------
lr : 0.0003520365877844011
Epoch:[4] step : [400/429]
data_time: 16.707, batch_time: 17.420
Loss : 0.96199
----------------------------------------------
lr : 0.00010453658778440106
Epoch:[5] step : [100/429]
data_time: 0.050, batch_time: 0.726
Loss : 0.95804
----------------------------------------------
lr : 0.00010453658778440106
Epoch:[5] step : [200/429]
data_time: 0.047, batch_time: 0.734
Loss : 0.95778
----------------------------------------------
lr : 0.00010453658778440106
Epoch:[5] step : [300/429]
data_time: 3.535, batch_time: 4.262
Loss : 0.95560
----------------------------------------------
lr : 0.00010453658778440106
Epoch:[5] step : [400/429]
data_time: 16.183, batch_time: 16.886
Loss : 0.95380
======================================================
validation gogosing
validation loss -> 0.95380, 	 f1_score -> 0.59023
======================================================
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [100/429]
data_time: 9.344, batch_time: 10.029
Loss : 0.95624
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [200/429]
data_time: 3.875, batch_time: 4.588
Loss : 0.95323
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [300/429]
data_time: 0.049, batch_time: 0.720
Loss : 0.94880
----------------------------------------------
lr : 1e-05
Epoch:[6] step : [400/429]
data_time: 0.651, batch_time: 1.315
Loss : 0.94570
----------------------------------------------
lr : 0.00010453658778440102
Epoch:[7] step : [100/429]
data_time: 7.210, batch_time: 7.894
Loss : 0.95704
----------------------------------------------
lr : 0.00010453658778440102
Epoch:[7] step : [200/429]
data_time: 10.264, batch_time: 10.953
Loss : 0.95075
----------------------------------------------
lr : 0.00010453658778440102
Epoch:[7] step : [300/429]
data_time: 7.595, batch_time: 8.286
Loss : 0.95329
----------------------------------------------
lr : 0.00010453658778440102
Epoch:[7] step : [400/429]
data_time: 16.965, batch_time: 17.657
Loss : 0.95761
----------------------------------------------
lr : 0.0003520365877844012
Epoch:[8] step : [100/429]
data_time: 0.058, batch_time: 0.733
Loss : 0.95913
----------------------------------------------
lr : 0.0003520365877844012
Epoch:[8] step : [200/429]
data_time: 0.049, batch_time: 0.721
Loss : 0.95886
----------------------------------------------
lr : 0.0003520365877844012
Epoch:[8] step : [300/429]
data_time: 0.048, batch_time: 0.721
Loss : 0.95752
----------------------------------------------
lr : 0.0003520365877844012
Epoch:[8] step : [400/429]
data_time: 0.041, batch_time: 0.709
Loss : 0.95854
----------------------------------------------
lr : 0.0006579634122155993
Epoch:[9] step : [100/429]
data_time: 4.430, batch_time: 5.118
Loss : 0.96133
----------------------------------------------
lr : 0.0006579634122155993
Epoch:[9] step : [200/429]
data_time: 0.033, batch_time: 0.708
Loss : 0.96394
----------------------------------------------
lr : 0.0006579634122155993
Epoch:[9] step : [300/429]
data_time: 16.926, batch_time: 17.657
Loss : 0.96139
----------------------------------------------
lr : 0.0006579634122155993
Epoch:[9] step : [400/429]
data_time: 0.044, batch_time: 0.712
Loss : 0.95648
----------------------------------------------
lr : 0.0009054634122155996
Epoch:[10] step : [100/429]
data_time: 2.750, batch_time: 3.450
Loss : 0.95835
----------------------------------------------
lr : 0.0009054634122155996
Epoch:[10] step : [200/429]
data_time: 7.069, batch_time: 7.775
Loss : 0.96432
----------------------------------------------
lr : 0.0009054634122155996
Epoch:[10] step : [300/429]
data_time: 16.842, batch_time: 17.551
Loss : 0.96028
----------------------------------------------
lr : 0.0009054634122155996
Epoch:[10] step : [400/429]
data_time: 20.236, batch_time: 20.948
Loss : 0.95952
======================================================
validation gogosing
validation loss -> 0.97261, 	 f1_score -> 0.46284
======================================================
----------------------------------------------
lr : 0.0010000000000000007
Epoch:[11] step : [100/429]
data_time: 0.037, batch_time: 0.703
Loss : 0.96247
----------------------------------------------
lr : 0.0010000000000000007
Epoch:[11] step : [200/429]
data_time: 0.045, batch_time: 0.710
Loss : 0.96480
----------------------------------------------
lr : 0.0010000000000000007
Epoch:[11] step : [300/429]
data_time: 16.065, batch_time: 16.752
Loss : 0.96523
----------------------------------------------
lr : 0.0010000000000000007
Epoch:[11] step : [400/429]
data_time: 0.049, batch_time: 0.717
Loss : 0.95999
----------------------------------------------
lr : 0.0009054634122155999
Epoch:[12] step : [100/429]
data_time: 0.043, batch_time: 0.714
Loss : 0.95988
----------------------------------------------
lr : 0.0009054634122155999
Epoch:[12] step : [200/429]
data_time: 16.503, batch_time: 17.210
Loss : 0.95848
----------------------------------------------
lr : 0.0009054634122155999
Epoch:[12] step : [300/429]
data_time: 16.909, batch_time: 17.624
Loss : 0.96248
----------------------------------------------
lr : 0.0009054634122155999
Epoch:[12] step : [400/429]
data_time: 9.670, batch_time: 10.366
Loss : 0.95910
----------------------------------------------
lr : 0.0006579634122155996
Epoch:[13] step : [100/429]
data_time: 17.239, batch_time: 17.973
Loss : 0.95503
----------------------------------------------
lr : 0.0006579634122155996
Epoch:[13] step : [200/429]
data_time: 8.195, batch_time: 8.889
Loss : 0.95461
